[//]: # (Image References)
[image_0]: ./misc/rover_image.jpg
# Search and Sample Return Project By AGKhalil
This is a writeup addressing each point of the project and how they were tackled.

#### 1. Run the functions provided in the notebook on test images (first with the test data provided, next on data you have recorded). Add/modify functions to allow for color selection of obstacles and rock samples.
The notebook has been an essential pillar for understanding the project in all its aspects. This project deals with how a robot can use opencv to analyze an image of its surroundings and behave accordingly. The rover first needs to breakdown the image and identify the navigable terrain, the obstacles, and the target stones. Afterwards, the rover transforms that image into a bird view perspective, which, as the robot keeps moving and obtaining imagery, maps the world surrounding it. Then the robot can decide how to behave based on that information. What the notebook does is that it provides a walkthrough through each aspect necessary to accomplish the mapping process.

The first function that had to be modified in the notebook is "Perspective Transform". This function transforms the image viewed by the rover into a birds view image. The before and after images below describe what the function does. A grid is applied to the initial image to help planning the transformation process. Each square is one meter squared. *source* and *destination* are the four corner points corresponding to one meter squared on the initial and final images. The function takes in the source points and transforms them to a birds view.

![Source image](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen_Shot_2017-06-02_at_11.10.54_PM.png)

![Destination Image](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-02%20at%2011.10.59%20PM.png)


The second block of code modified is regarding color threshing. There are four different objects that can be in any image processed by the rover, road, obstacles, sky, and yellow rocks. Each object has different color composition; therefore, the rover can easily breakdown the image based on color thresholds and identify where the road is and where the yellow rocks are. There are three functions in charge of achieving this. *terrain_thresh* identifies navigable terrain, *obstacle_terrain* identifies obstacles, which could be big rocks, sky, or mountains, and *stone_thresh* which identifies the yellow rocks. Each function outputs a binary image. Ones are given to where the threshold holds true and zeros where otherwise. The first image below is what the rover sees, the second is the birds view of that, and the rest are the outputs of the color threshold functions. Each showing the navigable terrain, the obstacles, and the yellow rocks, respectively.

![Original image](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-02%20at%2011.22.17%20PM.png)

![Warped image](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-02%20at%2011.22.20%20PM.png)

![Color Threshold Images](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-02%20at%2011.21.56%20PM.png)

After the thresholds are applied the images are mapped from the relative axes of the rover onto the universal axes of the map. This is achieved through *rover_coords*, *rotate_pix*, *translate_pix* and *pix_to_world*. *rover_coords* takes in the output of the threshold functions and maps the binary images onto the relative axes of the rover. *rotate_pix* rotates the relative map to match the universal axes based on the rovers yaw angle. *translate_pix* moves the points to the origins of the universal map, which is where the rover currently is. *pix_to_world* combines all these functions together and applies them. An additional function is identified here, *to_polar_coords*, which transforms the rovers map coordinates into polar coordinates. This allows for easy identification of how far things are relative to the rover as well as their bearings. The images below provide a good demonstration of what these functions do. The arrow in image four is created by taking the mean of all the binary true pixels in the image.

![Coordinate transformations](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-02%20at%2011.31.38%20PM.png)

####2. Populate the *process_image()* function with the appropriate analysis steps to map pixels identifying navigable terrain, obstacles and rock samples into a world map. Run *process_image()* on your test data using the moviepy functions provided to create video output of your result.
This function combines all of the functions mentioned above to and then compiles the images in a video using the moviepy functions. First, the source and destination points for *perspect_transfrom* are identified. Second, the input image is transformed to birds view using *perspect_transfrom*. Third, the image is processed three times by *terrain_thresh, obstacle_thresh,* and *stone_thresh* to provide binary images for each. Fourth, each binary image is converted to rover-centric coordinates using *rover_coords*. Fifth, *pix_to_world* converts all rover-centric coordinates to world coordinates. Sixth, the worldmap in the rover's *data* is updated. Seventh, the output images are outputted to the screen. This entire process is repeated over and over again for all the images recorded while the simulation was on. These images are then compiled together into a video as shown below.

####3. Fill in the *perception_step()* (at the bottom of the *perception.py* script) and *decision_step()* (in *decision.py*) functions in the autonomous mapping scripts and an explanation is provided in the writeup of how and why these functions were modified as they were.
*perception_step()* is incredibly similar to *process_image()*. There is a slight difference of course; *perception_step()* updates the rover's binary view as well as the truth map. *perception_step()* also has a bit of logic related to the fidelity of the robot. The rover's fidelity is defined as how accurate the mapping images are. The main issue here is that the rover moves in all three dimensions, while ideally we want it to move only in one, yaw. This movement in roll and pitch are the main reason for the inaccuracy. Therefore, *perception_step()* has a conditional statement prior to updating the truthmap, if the roll and pitch are not within half of a degree from zero, don't update. This helped in maintaining the fidelity in the high 80s.

*decision_step()* hasn't been modified at all. To get this project to work all that was needed to be modified were *perception_step()* and *drive_rover.py*. *drive_rover.py* was modified to reduce the rover's speed and brake, which decrease any sudden changes in roll and pitch. Furthermore, the thresholds for stopping and moving forward have been modified. Both were increased to prevent the rover from getting too close to rocks or in tight spots where it would get stuck. This would have been a problem if the project was taken a step further to pick up the stones since they are all close to the walls.

####4. Launching in autonomous mode your rover can navigate and map autonomously. Explain your results and how you might improve them in your writeup.
Below is a screenshot of a successful run!

![Success](https://github.com/AGKhalil/RoboND-Rover-Project/blob/master/Writeup_images/Screen%20Shot%202017-06-03%20at%2012.52.00%20AM.png)

The robot can move pretty well and avoid being stuck due to the thresholds placed on it in terms of when to move and stop. Since both thresholds are pretty high, the rover will not move into areas where the navigable terrain is low in pixels. However, this would be a problem if the rover starts collecting stones since they are all very close to the walls. In the future, I would find a better way of avoiding obstacles by optimizing the thresholds.

The fidelity of the rover is pretty high. Nevertheless, to improve it more, I would place thresholds on the distance of pixels from the rover. If the pixels are far enough, don't take them into account. This would reduce the amount of error; however, it would also reduce the speed by which the rover could cover the entire map since its view would be limited. It would be an unnecessary tradeoff since the fidelity is already pretty high. Yet in a situation where there are many more obstacles and objects surrounding the rover, the tradeoff would be necessary for a high fidelity.

I would have liked to spend more time on collecting the stones. Specifically without hugging the wall, but this is a more advanced and complex task. Maybe in the future when I am more equipped. This has been a very educational project, I am excited for what's to come!